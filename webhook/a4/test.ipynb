{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.testing import assert_close\n",
    "\n",
    "from ref.modeling import (\n",
    "    MLPActivationType,\n",
    "    AttnQKVPackFormat,\n",
    "    AttnQKVLayout,\n",
    "    TransformerConfig,\n",
    "    TransformerDecoderKVCache,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoderBlock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate toy test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task1 - case1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " None,\n",
       " None,\n",
       " True,\n",
       " (tensor([[[[-0.6484, -0.7058,  0.6432,  1.4788]],\n",
       "  \n",
       "           [[ 1.1918, -0.1446,  0.4847,  0.6921]],\n",
       "  \n",
       "           [[-1.3929,  0.7623,  0.8387, -1.0450]]]]),\n",
       "  tensor([[[[ 1.1097,  0.3953,  1.1804, -0.8989]],\n",
       "  \n",
       "           [[-0.8313,  0.4680,  2.2700,  0.0743]],\n",
       "  \n",
       "           [[-0.8931, -0.9201, -0.0213,  1.7711]]]]),\n",
       "  None),\n",
       " None,\n",
       " False,\n",
       " None,\n",
       " None,\n",
       " True,\n",
       " (tensor([[[[-2.0157,  2.0106,  0.0583,  0.0656]],\n",
       "  \n",
       "           [[ 0.4625, -0.1692,  0.3719,  1.4709]],\n",
       "  \n",
       "           [[-0.1568, -2.8720,  1.9054, -0.1457]]]]),\n",
       "  tensor([[[[-1.6534,  2.2517,  0.9501,  2.2385]],\n",
       "  \n",
       "           [[-1.8826, -1.0217, -0.2169, -1.0115]],\n",
       "  \n",
       "           [[ 0.1614, -0.0939,  1.7723, -0.0284]]]]),\n",
       "  None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_transformer_decoder_kvcache_args(\n",
    "    b: int,\n",
    "    nh: int,\n",
    "    hd: int,\n",
    "    qkv_layout: AttnQKVLayout,\n",
    "    ops: list,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    device: str = \"cpu\",\n",
    "    seed: int = 42,\n",
    "):\n",
    "    input_tensors = []\n",
    "    \n",
    "    for i, op in enumerate(ops):\n",
    "        if op['op'] in (\"set\", \"append\"):\n",
    "            s, seqlens = op['s'], op['seqlens']\n",
    "            \n",
    "            torch.manual_seed(seed + i)\n",
    "            k = torch.randn(b, s, nh, hd, dtype=dtype, device=device)\n",
    "            v = torch.randn_like(k)\n",
    "            cu_seqlens = None\n",
    "            \n",
    "            match qkv_layout:\n",
    "                case AttnQKVLayout.SBHD:\n",
    "                    k, v = [x.transpose(0, 1) for x in (k, v)]\n",
    "                case AttnQKVLayout.THD:\n",
    "                    assert b == 1, \"b should be equal to 1 when qkv_layout is THD\"\n",
    "                    assert seqlens is not None, \"seqlens must be given when qkv_layout is THD\"\n",
    "                    k, v = [x.squeeze(0) for x in (k, v)]\n",
    "                    cu_seqlens = torch.concat([\n",
    "                            torch.zeros(1, dtype=torch.int32, device=device),\n",
    "                            torch.tensor(seqlens, dtype=torch.int32, device=device).cumsum(dim=0)\n",
    "                    ], dim=0)\n",
    "                    assert cu_seqlens[-1] == (t:=b*s), f\"The sum of seqlens ({cu_seqlens[-1]}) != length ({t})\"\n",
    "            input_tensors.append((k, v, cu_seqlens))\n",
    "        else:\n",
    "            input_tensors.append(None)\n",
    "        \n",
    "    return input_tensors\n",
    "           \n",
    "b, nh, hd = 1, 1, 4\n",
    "layout = AttnQKVLayout.BSHD\n",
    "num_layers = 2\n",
    "ops = [\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"set\",\n",
    "        \"layer_idx\": 1,\n",
    "        \"s\": 3,\n",
    "        \"seqlens\": None,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"set\",\n",
    "        \"layer_idx\": 0,\n",
    "        \"s\": 2,\n",
    "        \"seqlens\": None,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"get\",\n",
    "        \"layer_idx\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"reset\",\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"append\",\n",
    "        \"layer_idx\": 0,\n",
    "        \"s\": 1,\n",
    "        \"seqlens\": None,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"append\",\n",
    "        \"layer_idx\": 0,\n",
    "        \"s\": 2,\n",
    "        \"seqlens\": None,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"get\",\n",
    "        \"layer_idx\": 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "input_tensors = construct_transformer_decoder_kvcache_args(\n",
    "    b, nh, hd, \n",
    "    layout,\n",
    "    ops,\n",
    ")\n",
    "\n",
    "kv_cache = TransformerDecoderKVCache(\n",
    "    qkv_layout=layout,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "outputs_ref = []\n",
    "\n",
    "for i, (op, input_tensor) in enumerate(zip(ops, input_tensors)):\n",
    "    match op['op']:\n",
    "        case \"reset\":\n",
    "            kv_cache.reset()\n",
    "            outputs_ref.append(None)\n",
    "        case \"has\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            outputs_ref.append(kv_cache.has(layer_idx))\n",
    "        case \"get\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            outputs_ref.append(kv_cache.get(layer_idx))\n",
    "        case \"set\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            k, v, cu_seqlens = input_tensor\n",
    "            kv_cache.set(layer_idx, k, v, cu_seqlens=cu_seqlens)\n",
    "            outputs_ref.append(None)\n",
    "        case \"append\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            k, v, cu_seqlens = input_tensor\n",
    "            kv_cache.append(layer_idx, k, v, cu_seqlens=cu_seqlens)\n",
    "            outputs_ref.append(None)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown operation: {op['op']}\")\n",
    "\n",
    "outputs_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task1 - case2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " None,\n",
       " None,\n",
       " True,\n",
       " (tensor([[[ 1.5862,  1.1253,  1.8306,  0.1129]],\n",
       "  \n",
       "          [[ 0.4976,  1.5010, -0.1413, -0.3522]],\n",
       "  \n",
       "          [[-0.1643, -1.1651, -0.4089, -0.5252]],\n",
       "  \n",
       "          [[-1.3153,  0.6031, -0.8124,  0.5920]]]),\n",
       "  tensor([[[-1.2266, -0.9598,  1.7118, -0.0146]],\n",
       "  \n",
       "          [[ 0.4252, -1.3446,  1.6114,  0.5914]],\n",
       "  \n",
       "          [[ 0.1644,  1.2514,  0.5173, -0.8078]],\n",
       "  \n",
       "          [[-2.0788,  0.6370,  1.3824, -0.9156]]]),\n",
       "  tensor([0, 1, 2, 4])),\n",
       " None,\n",
       " True,\n",
       " (tensor([[[-0.0166, -0.4668,  2.0909,  0.6149]],\n",
       "  \n",
       "          [[ 0.3083, -0.2947, -0.7662, -0.9962]],\n",
       "  \n",
       "          [[-1.4624,  0.7523, -1.7173,  0.5757]],\n",
       "  \n",
       "          [[-0.2345, -0.5367,  1.1296,  0.1054]],\n",
       "  \n",
       "          [[-0.3630,  1.5822, -0.4430,  1.8462]],\n",
       "  \n",
       "          [[ 0.6040,  1.1914,  0.3525,  0.2941]],\n",
       "  \n",
       "          [[-0.4772, -1.8291, -0.6145,  1.0282]],\n",
       "  \n",
       "          [[ 0.5197, -0.1634, -0.0875,  0.6146]]]),\n",
       "  tensor([[[-0.7771, -0.4484, -1.1668,  0.5006]],\n",
       "  \n",
       "          [[ 0.0139,  0.6564,  0.4846, -0.2549]],\n",
       "  \n",
       "          [[ 0.3034,  0.7770, -2.0360,  0.3562]],\n",
       "  \n",
       "          [[-0.7603, -1.6943, -0.2596,  0.8847]],\n",
       "  \n",
       "          [[-0.8256,  0.7988, -0.3005, -0.3062]],\n",
       "  \n",
       "          [[ 0.8027, -0.6474,  0.4054, -0.5901]],\n",
       "  \n",
       "          [[ 0.4163, -0.5947, -0.2367, -1.8343]],\n",
       "  \n",
       "          [[-0.5833, -0.6801, -1.2947,  0.9606]]]),\n",
       "  tensor([0, 3, 6, 8]))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_transformer_decoder_kvcache_args(\n",
    "    b: int,\n",
    "    nh: int,\n",
    "    hd: int,\n",
    "    qkv_layout: AttnQKVLayout,\n",
    "    ops: list,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    device: str = \"cpu\",\n",
    "    seed: int = 42,\n",
    "):\n",
    "    input_tensors = []\n",
    "    \n",
    "    for i, op in enumerate(ops):\n",
    "        if op['op'] in (\"set\", \"append\"):\n",
    "            s, seqlens = op['s'], op['seqlens']\n",
    "            \n",
    "            torch.manual_seed(seed + i)\n",
    "            k = torch.randn(b, s, nh, hd, dtype=dtype, device=device)\n",
    "            v = torch.randn_like(k)\n",
    "            cu_seqlens = None\n",
    "            \n",
    "            match qkv_layout:\n",
    "                case AttnQKVLayout.SBHD:\n",
    "                    k, v = [x.transpose(0, 1) for x in (k, v)]\n",
    "                case AttnQKVLayout.THD:\n",
    "                    assert b == 1, \"b should be equal to 1 when qkv_layout is THD\"\n",
    "                    assert seqlens is not None, \"seqlens must be given when qkv_layout is THD\"\n",
    "                    k, v = [x.squeeze(0) for x in (k, v)]\n",
    "                    cu_seqlens = torch.concat([\n",
    "                            torch.zeros(1, dtype=torch.int32, device=device),\n",
    "                            torch.tensor(seqlens, dtype=torch.int32, device=device).cumsum(dim=0)\n",
    "                    ], dim=0)\n",
    "                    assert cu_seqlens[-1] == (t:=b*s), f\"The sum of seqlens ({cu_seqlens[-1]}) != length ({t})\"\n",
    "            input_tensors.append((k, v, cu_seqlens))\n",
    "        else:\n",
    "            input_tensors.append(None)\n",
    "        \n",
    "    return input_tensors\n",
    "           \n",
    "b, nh, hd = 1, 1, 4\n",
    "layout = AttnQKVLayout.THD\n",
    "num_layers = 2\n",
    "ops = [\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"set\",\n",
    "        \"layer_idx\": 1,\n",
    "        \"s\": 5,\n",
    "        \"seqlens\": [2,2,1],\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"append\",\n",
    "        \"layer_idx\": 0,\n",
    "        \"s\": 4,\n",
    "        \"seqlens\": [1,1,2],\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"get\",\n",
    "        \"layer_idx\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"append\",\n",
    "        \"layer_idx\": 1,\n",
    "        \"s\": 3,\n",
    "        \"seqlens\": [1,1,1],\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"has\",\n",
    "        \"layer_idx\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"op\": \"get\",\n",
    "        \"layer_idx\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "input_tensors = construct_transformer_decoder_kvcache_args(\n",
    "    b, nh, hd, \n",
    "    layout,\n",
    "    ops,\n",
    ")\n",
    "\n",
    "kv_cache = TransformerDecoderKVCache(\n",
    "    qkv_layout=layout,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "outputs_ref = []\n",
    "\n",
    "for i, (op, input_tensor) in enumerate(zip(ops, input_tensors)):\n",
    "    match op['op']:\n",
    "        case \"reset\":\n",
    "            kv_cache.reset()\n",
    "            outputs_ref.append(None)\n",
    "        case \"has\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            outputs_ref.append(kv_cache.has(layer_idx))\n",
    "        case \"get\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            outputs_ref.append(kv_cache.get(layer_idx))\n",
    "        case \"set\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            k, v, cu_seqlens = input_tensor\n",
    "            kv_cache.set(layer_idx, k, v, cu_seqlens=cu_seqlens)\n",
    "            outputs_ref.append(None)\n",
    "        case \"append\":\n",
    "            layer_idx = op['layer_idx']\n",
    "            k, v, cu_seqlens = input_tensor\n",
    "            kv_cache.append(layer_idx, k, v, cu_seqlens=cu_seqlens)\n",
    "            outputs_ref.append(None)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown operation: {op['op']}\")\n",
    "\n",
    "outputs_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task23 - case1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8086, -1.5312,  0.4062,  0.1719, -0.2471,  0.2041, -0.8789,\n",
      "          -0.3867],\n",
      "         [ 0.6016,  0.2676, -0.8516, -0.2891,  1.0000, -0.7812,  1.3750,\n",
      "           0.1187],\n",
      "         [ 0.8633, -0.7656,  0.8242, -1.1875, -0.0330, -0.0801,  0.0781,\n",
      "          -0.6484],\n",
      "         [-0.4746, -0.6680,  1.0547, -0.0359, -1.3203, -0.6719,  0.0415,\n",
      "          -0.6445],\n",
      "         [ 0.2637,  0.7070, -0.2188,  2.8906,  1.3672, -0.1084,  0.2402,\n",
      "           1.8359],\n",
      "         [ 1.7500, -0.0459, -0.3516, -0.0962, -1.7656, -0.9102, -0.5977,\n",
      "           0.6602],\n",
      "         [ 0.0393,  1.6094, -1.5078,  1.2656,  0.2227, -1.0312,  0.6055,\n",
      "          -0.4707],\n",
      "         [ 1.6406, -0.3086, -0.0693, -1.0469,  0.6211, -0.1455,  0.3242,\n",
      "           2.3906]]], dtype=torch.bfloat16) None None\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "PARAM_DEVICE = \"cpu\"\n",
    "PARAM_DTYPE = torch.float32\n",
    "\n",
    "b, s, h, v = 1, 8, 8, 10\n",
    "layout = AttnQKVLayout.BSHD\n",
    "pack_format = AttnQKVPackFormat.Q_K_V\n",
    "\n",
    "past_seqlen_kv = 0\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers=1,\n",
    "    hidden_size=8,\n",
    "    ffh_size=16,\n",
    "    max_seq_len=8,\n",
    "    param_dtype=PARAM_DTYPE,\n",
    "    param_device=PARAM_DEVICE,\n",
    "    init_base_seed=SEED,\n",
    "    \n",
    "    vocab_size=10,\n",
    "    vocab_init_mean=0.1,\n",
    "    vocab_init_std=1.1,\n",
    "    \n",
    "    rope_base=10000,\n",
    "    rope_ratio=1,\n",
    "    rope_dynamic=False,\n",
    "    \n",
    "    group_size=None,\n",
    "    eps=1e-5,\n",
    "    norm_init_range=(-1.1, 1.1),\n",
    "    \n",
    "    proj_init_seed=SEED,\n",
    "    proj_init_mean=0.1,\n",
    "    proj_init_std=1.1,\n",
    "    lm_head_tied=False,\n",
    "    \n",
    "    online_attn_block_size=4,\n",
    "    head_dim=4,\n",
    "    num_q_head=2,\n",
    "    num_kv_head=1,\n",
    "    qkv_pack_format=AttnQKVPackFormat.Q_K_V,\n",
    "    qkv_layout=AttnQKVLayout.BSHD,\n",
    "    window_size=None,\n",
    "    causal=True,\n",
    "    softmax_dropout_rate=0.,\n",
    "    softmax_dropout_seed=SEED,\n",
    "    softmax_scale=None,\n",
    "    softmax_cap=None,\n",
    "    softmax_temp=1.,\n",
    "    softmax_clip_range=(0., 1.),\n",
    "    apply_qk_norm=False,\n",
    "    qk_norm_group_size=None,\n",
    "    \n",
    "    activation_type=MLPActivationType.SILU,\n",
    "    lora_rank=0,\n",
    "    lora_alpha=None,\n",
    "    lora_dropout_rate=0.,\n",
    "    lora_dropout_seed=SEED,\n",
    "    lora_init_base_seed=SEED,\n",
    "    \n",
    "    num_experts=None,\n",
    "    moe_topk=1,\n",
    "    gate_init_mean=0.,\n",
    "    gate_init_std=1.,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input = torch.randn(b, s, h, dtype=torch.bfloat16, device=\"cpu\")\n",
    "input_ids = torch.randint(0, v, (b, s), dtype=torch.int32, device=\"cpu\")\n",
    "\n",
    "if past_seqlen_kv > 0:\n",
    "    kv_cache = TransformerDecoderKVCache(\n",
    "        qkv_layout=layout,\n",
    "        num_layers=config.num_layers,\n",
    "    )\n",
    "    torch.manual_seed(42)\n",
    "    past_k = torch.randn(b, past_seqlen_kv, config.num_kv_head, config.head_dim, dtype=config.param_dtype, device=config.param_device)\n",
    "    past_v = torch.randn_like(past_k)\n",
    "    past_cu_seqlens = None\n",
    "    \n",
    "    if layout is AttnQKVLayout.SBHD:\n",
    "        past_k, past_v = [x.transpose(0, 1) for x in (past_k, past_v)]\n",
    "    elif layout is AttnQKVLayout.THD:\n",
    "        past_k, past_v = [x.squeeze(0) for x in (past_k, past_v)]\n",
    "        past_cu_seqlens = torch.tensor([0, 5, 9, s]).long()\n",
    "    \n",
    "    for layer_idx in range(config.num_layers):\n",
    "        kv_cache.set(layer_idx, past_k, past_v, cu_seqlens=past_cu_seqlens)\n",
    "\n",
    "else:\n",
    "    kv_cache = None\n",
    "\n",
    "if layout is AttnQKVLayout.THD:\n",
    "    if past_seqlen_kv > 0:\n",
    "        cu_seqlens = torch.tensor([0, 1, 2, 3]).long()\n",
    "    else:\n",
    "        cu_seqlens = torch.tensor([0, 5, 9, s]).long()\n",
    "else:\n",
    "    cu_seqlens = None\n",
    "\n",
    "print(input, cu_seqlens, kv_cache.get(0) if kv_cache else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayer(\n",
      "  (attn_pre_norm): GroupRMSNorm()\n",
      "  (rope): NTKAwareRoPE()\n",
      "  (attn): OnlineSlidingWindowAttn(\n",
      "    (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp_pre_norm): GroupRMSNorm()\n",
      "  (mlp): DenseMLPWithLoRA()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 8]),\n",
       " tensor([[[ -1.6328, -14.3125,  -4.7188,  -5.0938,   0.3770,  -1.6016,  -5.3750,\n",
       "             0.8789],\n",
       "          [  6.3438,  11.1250,  -1.8672,   6.3750,  -0.5703,   5.6250,  -0.2734,\n",
       "            -7.4688],\n",
       "          [  0.5391,  -7.1875,   1.2656,  -3.7812,   8.4375,  -4.0625,  -3.5312,\n",
       "             1.9922],\n",
       "          [  2.7031,  -0.1162,   3.5625,  -5.4062,   2.9531,  -1.2500,  -3.1094,\n",
       "             4.8125],\n",
       "          [ -3.8125,  -6.7188,   2.9688,   0.1011,   4.1562,  -9.5625,   4.6875,\n",
       "             5.2188],\n",
       "          [  5.3438,  -1.9375,   1.1562,  -7.5312,   2.4062,  -0.8555,  -6.0938,\n",
       "             8.6250],\n",
       "          [ -3.8906, -11.8125,   4.1875,  -6.4688,   6.9062, -16.2500,   4.6250,\n",
       "             6.8750],\n",
       "          [ -4.1562,   3.1719,  -1.3906,  -0.3086, -19.8750,   3.0156,   2.8125,\n",
       "             4.3125]]], dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerDecoderLayer(config)\n",
    "print(layer)\n",
    "\n",
    "output_layer = layer(input, cu_seqlens=cu_seqlens, kv_cache=kv_cache)\n",
    "output_layer.shape, output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderBlock(\n",
      "  (vocab_emb): ParallelVocabEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerDecoderLayer(\n",
      "      (attn_pre_norm): GroupRMSNorm()\n",
      "      (rope): NTKAwareRoPE()\n",
      "      (attn): OnlineSlidingWindowAttn(\n",
      "        (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp_pre_norm): GroupRMSNorm()\n",
      "      (mlp): DenseMLPWithLoRA()\n",
      "    )\n",
      "  )\n",
      "  (kv_cache): TransformerDecoderKVCache()\n",
      "  (final_norm): GroupRMSNorm()\n",
      "  (lm_head): Linear(in_features=8, out_features=10, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 10]),\n",
       " tensor([[[-1.2925,  2.5357, -0.7369, -0.7364, -0.5004,  0.9618, -0.5549,\n",
       "            1.1369, -0.5655,  0.4905],\n",
       "          [-3.4057,  1.3570, -1.5379, -0.7549,  1.6519,  1.2669,  0.6092,\n",
       "            1.7056, -1.2644,  0.6840],\n",
       "          [-1.7036,  3.1648, -0.5522,  0.9891, -0.1064,  1.8245, -0.3872,\n",
       "            2.3809,  0.6620,  1.5979],\n",
       "          [ 3.0575,  2.4082, -1.1623, -0.3622, -1.9437,  1.4024, -0.7102,\n",
       "           -1.4185, -0.5156,  1.2056],\n",
       "          [-3.4312,  1.0153, -1.4652, -1.0645,  1.6408,  1.0096,  0.6544,\n",
       "            1.4626, -1.4687,  0.4301],\n",
       "          [-3.1556,  0.6621, -1.4333, -1.3881,  1.5312,  0.7911,  0.6917,\n",
       "            1.0304, -1.7093,  0.1825],\n",
       "          [-5.0665,  0.8959, -2.1492,  0.0602,  3.3647,  2.4823,  1.7351,\n",
       "            2.9604, -1.4029,  1.7156],\n",
       "          [ 1.0452,  3.8170, -1.2443,  1.9853, -0.2522,  2.6731, -0.7634,\n",
       "            1.0764,  0.5081,  1.9799]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerDecoderBlock(config)\n",
    "block = block.train() if config.online_attn_block_size is not None else block.eval()\n",
    "if kv_cache is not None:\n",
    "    block.set_kv_cache(kv_cache)\n",
    "print(block)\n",
    "\n",
    "output_block = block(input_ids, cu_seqlens=cu_seqlens)\n",
    "output_block.shape, output_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task23 - case2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3359,  0.1289,  0.2344,  0.2305, -1.1250, -0.1865,  2.2031,\n",
      "          -0.6367]]], dtype=torch.bfloat16) tensor([[5]], dtype=torch.int32) None (tensor([[[[ 1.9269,  1.4873,  0.9007, -2.1055]]],\n",
      "\n",
      "\n",
      "        [[[-0.7581,  1.0783,  0.8008,  1.6806]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3559, -0.6866, -0.4934,  0.2415]]],\n",
      "\n",
      "\n",
      "        [[[-0.2316,  0.0418, -0.2516,  0.8599]]],\n",
      "\n",
      "\n",
      "        [[[-0.3097, -0.3957,  0.8034, -0.6216]]]]), tensor([[[[ 0.3189, -0.4245,  0.3057, -0.7746]]],\n",
      "\n",
      "\n",
      "        [[[-0.8371, -0.9224,  1.8113,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3672,  0.1754,  1.3852, -0.4459]]],\n",
      "\n",
      "\n",
      "        [[[-1.2024,  0.7078, -1.0759,  0.5357]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1754,  0.5612, -0.4527, -0.7718]]]]), None)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "PARAM_DEVICE = \"cpu\"\n",
    "PARAM_DTYPE = torch.float32\n",
    "\n",
    "b, s, h, v = 1, 1, 8, 10\n",
    "layout = AttnQKVLayout.SBHD\n",
    "pack_format = AttnQKVPackFormat.Q_KV\n",
    "\n",
    "past_seqlen_kv = 5\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers=2,\n",
    "    hidden_size=8,\n",
    "    ffh_size=16,\n",
    "    max_seq_len=16,\n",
    "    param_dtype=PARAM_DTYPE,\n",
    "    param_device=PARAM_DEVICE,\n",
    "    init_base_seed=SEED,\n",
    "    \n",
    "    vocab_size=10,\n",
    "    vocab_init_mean=0.1,\n",
    "    vocab_init_std=1.1,\n",
    "    \n",
    "    rope_base=10000,\n",
    "    rope_ratio=1,\n",
    "    rope_dynamic=False,\n",
    "    \n",
    "    group_size=None,\n",
    "    eps=1e-5,\n",
    "    norm_init_range=(-1.1, 1.1),\n",
    "    \n",
    "    proj_init_seed=SEED,\n",
    "    proj_init_mean=0.1,\n",
    "    proj_init_std=1.1,\n",
    "    lm_head_tied=False,\n",
    "    \n",
    "    online_attn_block_size=None,\n",
    "    head_dim=4,\n",
    "    num_q_head=2,\n",
    "    num_kv_head=1,\n",
    "    qkv_pack_format=AttnQKVPackFormat.Q_KV,\n",
    "    qkv_layout=AttnQKVLayout.SBHD,\n",
    "    window_size=None,\n",
    "    causal=True,\n",
    "    softmax_dropout_rate=0.,\n",
    "    softmax_dropout_seed=SEED,\n",
    "    softmax_scale=None,\n",
    "    softmax_cap=None,\n",
    "    softmax_temp=1.,\n",
    "    softmax_clip_range=(0., 1.),\n",
    "    apply_qk_norm=False,\n",
    "    qk_norm_group_size=None,\n",
    "    \n",
    "    activation_type=MLPActivationType.SILU,\n",
    "    lora_rank=0,\n",
    "    lora_alpha=None,\n",
    "    lora_dropout_rate=0.,\n",
    "    lora_dropout_seed=SEED,\n",
    "    lora_init_base_seed=SEED,\n",
    "    \n",
    "    num_experts=None,\n",
    "    moe_topk=1,\n",
    "    gate_init_mean=0.,\n",
    "    gate_init_std=1.,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input = torch.randn(b, s, h, dtype=torch.bfloat16, device=\"cpu\")\n",
    "input_ids = torch.randint(0, v, (b, s), dtype=torch.int32, device=\"cpu\")\n",
    "\n",
    "if past_seqlen_kv > 0:\n",
    "    kv_cache = TransformerDecoderKVCache(\n",
    "        qkv_layout=layout,\n",
    "        num_layers=config.num_layers,\n",
    "    )\n",
    "    for layer_idx in range(config.num_layers):\n",
    "        torch.manual_seed(42 + layer_idx)\n",
    "        past_k = torch.randn(b, past_seqlen_kv, config.num_kv_head, config.head_dim, dtype=config.param_dtype, device=config.param_device)\n",
    "        past_v = torch.randn_like(past_k)\n",
    "        past_cu_seqlens = None\n",
    "        \n",
    "        if layout is AttnQKVLayout.SBHD:\n",
    "            past_k, past_v = [x.transpose(0, 1) for x in (past_k, past_v)]\n",
    "        elif layout is AttnQKVLayout.THD:\n",
    "            past_k, past_v = [x.squeeze(0) for x in (past_k, past_v)]\n",
    "            past_cu_seqlens = torch.tensor([0, 5, 9, past_seqlen_kv]).long()\n",
    "        \n",
    "        kv_cache.set(layer_idx, past_k, past_v, cu_seqlens=past_cu_seqlens)\n",
    "\n",
    "else:\n",
    "    kv_cache = None\n",
    "\n",
    "if layout is AttnQKVLayout.THD:\n",
    "    if past_seqlen_kv > 0:\n",
    "        cu_seqlens = torch.tensor([0, 1, 2, 3]).long()\n",
    "    else:\n",
    "        cu_seqlens = torch.tensor([0, 5, 9, s]).long()\n",
    "else:\n",
    "    cu_seqlens = None\n",
    "\n",
    "print(input, input_ids, cu_seqlens, kv_cache.get(0) if kv_cache else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayer(\n",
      "  (attn_pre_norm): GroupRMSNorm()\n",
      "  (rope): NTKAwareRoPE()\n",
      "  (attn): OfflineSlidingWindowAttn(\n",
      "    (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp_pre_norm): GroupRMSNorm()\n",
      "  (mlp): DenseMLPWithLoRA()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 8]),\n",
       " tensor([[[ 2.1250, -6.6875,  3.5625, -2.6719,  3.8750, -7.8438,  2.1875,\n",
       "           -0.7852]]], dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerDecoderLayer(config)\n",
    "print(layer)\n",
    "\n",
    "output_layer = layer(input.clone(), cu_seqlens=cu_seqlens.clone() if cu_seqlens is not None else None, kv_cache=deepcopy(kv_cache) if kv_cache else None)\n",
    "output_layer.shape, output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "TransformerDecoderBlock(\n",
      "  (vocab_emb): ParallelVocabEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerDecoderLayer(\n",
      "      (attn_pre_norm): GroupRMSNorm()\n",
      "      (rope): NTKAwareRoPE()\n",
      "      (attn): OfflineSlidingWindowAttn(\n",
      "        (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp_pre_norm): GroupRMSNorm()\n",
      "      (mlp): DenseMLPWithLoRA()\n",
      "    )\n",
      "  )\n",
      "  (kv_cache): TransformerDecoderKVCache()\n",
      "  (final_norm): GroupRMSNorm()\n",
      "  (lm_head): Linear(in_features=8, out_features=10, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 10]),\n",
       " tensor([[[-0.0713, -0.7954,  1.0491, -4.1084, -1.7625, -1.1286, -0.6247,\n",
       "           -1.1881, -2.9059, -3.9017]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerDecoderBlock(config)\n",
    "block = block.train() if config.online_attn_block_size is not None else block.eval()\n",
    "print(block.training)\n",
    "if kv_cache is not None:\n",
    "    block.set_kv_cache(kv_cache=deepcopy(kv_cache))\n",
    "print(block)\n",
    "\n",
    "output_block = block(input_ids.clone(), cu_seqlens=cu_seqlens.clone() if cu_seqlens is not None else None)\n",
    "output_block.shape, output_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### task23 - case3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8086, -1.5312,  0.4062,  0.1719, -0.2471,  0.2041, -0.8789,\n",
      "          -0.3867],\n",
      "         [ 0.5664,  0.2363,  0.4863,  1.1719,  1.4531, -0.8906,  0.1543,\n",
      "           0.8242],\n",
      "         [-2.1719,  1.3516,  0.2754, -0.1128, -0.7969,  1.3438,  0.3750,\n",
      "          -1.1328]]], dtype=torch.bfloat16) tensor([0, 1, 2, 3]) (tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055]],\n",
      "\n",
      "        [[ 0.6784, -1.2345, -0.0431, -1.6047]],\n",
      "\n",
      "        [[-0.7521,  1.6487, -0.3925, -1.4036]],\n",
      "\n",
      "        [[-0.7279, -0.5594, -0.7688,  0.7624]],\n",
      "\n",
      "        [[ 1.6423, -0.1596, -0.4974,  0.4396]],\n",
      "\n",
      "        [[-0.7581,  1.0783,  0.8008,  1.6806]],\n",
      "\n",
      "        [[ 1.2791,  1.2964,  0.6105,  1.3347]],\n",
      "\n",
      "        [[-0.2316,  0.0418, -0.2516,  0.8599]],\n",
      "\n",
      "        [[-1.3847, -0.8712, -0.2234,  1.7174]],\n",
      "\n",
      "        [[ 0.3189, -0.4245,  0.3057, -0.7746]],\n",
      "\n",
      "        [[-1.5576,  0.9956, -0.8798, -0.6011]],\n",
      "\n",
      "        [[-1.2742,  2.1228, -1.2347, -0.4879]]]), tensor([[[-0.9138, -0.6581,  0.0780,  0.5258]],\n",
      "\n",
      "        [[-0.4880,  1.1914, -0.8140, -0.7360]],\n",
      "\n",
      "        [[-1.4032,  0.0360, -0.0635,  0.6756]],\n",
      "\n",
      "        [[-0.0978,  1.8446, -1.1845,  1.3835]],\n",
      "\n",
      "        [[ 1.4451,  0.8564,  2.2181,  0.5232]],\n",
      "\n",
      "        [[ 0.3466, -0.1973, -1.0546,  1.2780]],\n",
      "\n",
      "        [[-0.1722,  0.5238,  0.0566,  0.4263]],\n",
      "\n",
      "        [[ 0.5750, -0.6417, -2.2064, -0.7508]],\n",
      "\n",
      "        [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "        [[ 0.5362,  0.5246,  1.1412,  0.0516]],\n",
      "\n",
      "        [[ 0.7440, -0.4816, -1.0495,  0.6039]],\n",
      "\n",
      "        [[-1.7223, -0.8278,  1.3347,  0.4835]]]), tensor([ 0,  5,  9, 12]))\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "PARAM_DEVICE = \"cpu\"\n",
    "PARAM_DTYPE = torch.float32\n",
    "\n",
    "b, s, h, v = 1, 3, 8, 10\n",
    "layout = AttnQKVLayout.THD\n",
    "pack_format = AttnQKVPackFormat.Q_K_V\n",
    "\n",
    "past_seqlen_kv = 12\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers=3,\n",
    "    hidden_size=8,\n",
    "    ffh_size=16,\n",
    "    max_seq_len=16,\n",
    "    param_dtype=PARAM_DTYPE,\n",
    "    param_device=PARAM_DEVICE,\n",
    "    init_base_seed=SEED,\n",
    "    \n",
    "    vocab_size=10,\n",
    "    vocab_init_mean=0.1,\n",
    "    vocab_init_std=1.1,\n",
    "    \n",
    "    rope_base=10000,\n",
    "    rope_ratio=1,\n",
    "    rope_dynamic=False,\n",
    "    \n",
    "    group_size=None,\n",
    "    eps=1e-5,\n",
    "    norm_init_range=(-1.1, 1.1),\n",
    "    \n",
    "    proj_init_seed=SEED,\n",
    "    proj_init_mean=0.1,\n",
    "    proj_init_std=1.1,\n",
    "    lm_head_tied=False,\n",
    "    \n",
    "    online_attn_block_size=None,\n",
    "    head_dim=4,\n",
    "    num_q_head=2,\n",
    "    num_kv_head=1,\n",
    "    qkv_pack_format=AttnQKVPackFormat.Q_K_V,\n",
    "    qkv_layout=AttnQKVLayout.THD,\n",
    "    window_size=None,\n",
    "    causal=True,\n",
    "    softmax_dropout_rate=0.,\n",
    "    softmax_dropout_seed=SEED,\n",
    "    softmax_scale=None,\n",
    "    softmax_cap=None,\n",
    "    softmax_temp=1.,\n",
    "    softmax_clip_range=(0., 1.),\n",
    "    apply_qk_norm=False,\n",
    "    qk_norm_group_size=None,\n",
    "    \n",
    "    activation_type=MLPActivationType.SILU,\n",
    "    lora_rank=0,\n",
    "    lora_alpha=None,\n",
    "    lora_dropout_rate=0.,\n",
    "    lora_dropout_seed=SEED,\n",
    "    lora_init_base_seed=SEED,\n",
    "    \n",
    "    num_experts=None,\n",
    "    moe_topk=1,\n",
    "    gate_init_mean=0.,\n",
    "    gate_init_std=1.,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input = torch.randn(b, s, h, dtype=torch.bfloat16, device=\"cpu\")\n",
    "input_ids = torch.randint(0, v, (b, s), dtype=torch.int32, device=\"cpu\")\n",
    "\n",
    "if past_seqlen_kv > 0:\n",
    "    kv_cache = TransformerDecoderKVCache(\n",
    "        qkv_layout=layout,\n",
    "        num_layers=config.num_layers,\n",
    "    )\n",
    "    for layer_idx in range(config.num_layers):\n",
    "        torch.manual_seed(42 + layer_idx)\n",
    "        past_k = torch.randn(b, past_seqlen_kv, config.num_kv_head, config.head_dim, dtype=config.param_dtype, device=config.param_device)\n",
    "        past_v = torch.randn_like(past_k)\n",
    "        past_cu_seqlens = None\n",
    "        \n",
    "        if layout is AttnQKVLayout.SBHD:\n",
    "            past_k, past_v = [x.transpose(0, 1) for x in (past_k, past_v)]\n",
    "        elif layout is AttnQKVLayout.THD:\n",
    "            past_k, past_v = [x.squeeze(0) for x in (past_k, past_v)]\n",
    "            past_cu_seqlens = torch.tensor([0, 5, 9, past_seqlen_kv]).long()\n",
    "        \n",
    "        kv_cache.set(layer_idx, past_k, past_v, cu_seqlens=past_cu_seqlens)\n",
    "\n",
    "else:\n",
    "    kv_cache = None\n",
    "\n",
    "if layout is AttnQKVLayout.THD:\n",
    "    if past_seqlen_kv > 0:\n",
    "        cu_seqlens = torch.tensor([0, 1, 2, 3]).long()\n",
    "    else:\n",
    "        cu_seqlens = torch.tensor([0, 5, 9, s]).long()\n",
    "else:\n",
    "    cu_seqlens = None\n",
    "\n",
    "print(input, cu_seqlens, kv_cache.get(0) if kv_cache else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayer(\n",
      "  (attn_pre_norm): GroupRMSNorm()\n",
      "  (rope): NTKAwareRoPE()\n",
      "  (attn): OfflineSlidingWindowAttn(\n",
      "    (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp_pre_norm): GroupRMSNorm()\n",
      "  (mlp): DenseMLPWithLoRA()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 8]),\n",
       " tensor([[[-1.8906, -9.5625, -4.8125, -1.9688,  4.5312, -2.0938, -4.7812,\n",
       "            0.2119],\n",
       "          [ 0.5859,  5.4062, -0.5000,  3.5938, -1.8828,  1.2734,  1.0469,\n",
       "           -3.1250],\n",
       "          [-6.5000, -8.9375,  4.3750, -0.1699, 11.8125, -7.8750,  0.5078,\n",
       "            2.1250]]], dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerDecoderLayer(config)\n",
    "print(layer)\n",
    "\n",
    "output_layer = layer(input.clone(), cu_seqlens=cu_seqlens.clone() if cu_seqlens is not None else None, kv_cache=deepcopy(kv_cache) if kv_cache else None)\n",
    "output_layer.shape, output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "TransformerDecoderBlock(\n",
      "  (vocab_emb): ParallelVocabEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x TransformerDecoderLayer(\n",
      "      (attn_pre_norm): GroupRMSNorm()\n",
      "      (rope): NTKAwareRoPE()\n",
      "      (attn): OfflineSlidingWindowAttn(\n",
      "        (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp_pre_norm): GroupRMSNorm()\n",
      "      (mlp): DenseMLPWithLoRA()\n",
      "    )\n",
      "  )\n",
      "  (kv_cache): TransformerDecoderKVCache()\n",
      "  (final_norm): GroupRMSNorm()\n",
      "  (lm_head): Linear(in_features=8, out_features=10, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 10]),\n",
       " tensor([[[-5.7833e+00, -4.1559e+00, -7.2531e-01, -1.2458e+00,  5.3105e+00,\n",
       "           -1.1368e+00,  2.0874e+00,  3.9418e-01, -3.1351e+00, -1.5997e+00],\n",
       "          [ 5.1554e-01, -2.8597e+00,  7.3401e-01,  7.7610e-01,  5.3917e-01,\n",
       "           -1.0834e+00,  1.0781e+00, -7.7173e-01,  1.2294e+00,  1.4850e-01],\n",
       "          [ 8.8537e-01,  1.7028e+00, -5.3950e-03,  4.7097e-01, -1.2100e+00,\n",
       "            1.1996e+00, -4.4219e-01,  2.1394e-01, -2.2099e-02,  1.5710e+00]]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerDecoderBlock(config)\n",
    "block = block.train() if config.online_attn_block_size is not None else block.eval()\n",
    "print(block.training)\n",
    "if kv_cache is not None:\n",
    "    block.set_kv_cache(kv_cache=deepcopy(kv_cache))\n",
    "print(block)\n",
    "\n",
    "output_block = block(input_ids.clone(), cu_seqlens=cu_seqlens.clone() if cu_seqlens is not None else None)\n",
    "output_block.shape, output_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transformer decoder, kv cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431,\n",
       "           -1.6047],\n",
       "          [-0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,\n",
       "            0.7624],\n",
       "          [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,\n",
       "            1.6806],\n",
       "          [ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,\n",
       "            0.8599],\n",
       "          [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057,\n",
       "           -0.7746],\n",
       "          [-1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347,\n",
       "           -0.4879]]]),\n",
       " tensor([[3, 7, 0, 9, 0, 9]]),\n",
       " tensor([[6, 9, 5, 4, 8, 8]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l, v = 2, 10\n",
    "b, s, h, ffh = 1, 6, 8, 16\n",
    "hd = 4\n",
    "online_block_size = None # None to use offline attn\n",
    "\n",
    "layout = AttnQKVLayout.SBHD\n",
    "pack_format = AttnQKVPackFormat.QKV\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers=l,\n",
    "    hidden_size=h,\n",
    "    ffh_size=ffh,\n",
    "    max_seq_len=s,\n",
    "    vocab_size=v,\n",
    "    head_dim=hd,\n",
    "    num_q_head=h//hd,\n",
    "    num_kv_head=1,\n",
    "    qkv_layout=layout,\n",
    "    qkv_pack_format=pack_format,\n",
    "    online_attn_block_size=online_block_size,\n",
    ")\n",
    "\n",
    "if layout is AttnQKVLayout.THD:\n",
    "    cu_seqlens = torch.tensor([0, 5, 9, s])\n",
    "else:\n",
    "    cu_seqlens = None\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input = torch.randn(b, s, h)\n",
    "input_ids = torch.randint(0, v, (b, s))\n",
    "labels = torch.randint(0, v, (b, s))\n",
    "input, input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayer(\n",
      "  (attn_pre_norm): GroupRMSNorm()\n",
      "  (rope): NTKAwareRoPE()\n",
      "  (attn): OfflineSlidingWindowAttn(\n",
      "    (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp_pre_norm): GroupRMSNorm()\n",
      "  (mlp): DenseMLPWithLoRA()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 8]),\n",
       " tensor([[[ 4.1074,  0.3974, -2.1693, -2.3000,  3.7595,  2.1975, -0.6886,\n",
       "           -5.9105],\n",
       "          [-0.4448, -1.6368, -4.2078, -0.8758,  1.5075,  1.8875, -3.5614,\n",
       "           -0.5663],\n",
       "          [ 1.1051, -0.8874,  0.0672,  0.7068, -2.6837,  2.0918,  0.9942,\n",
       "            3.3176],\n",
       "          [ 1.6730,  0.2681,  0.7090,  1.6265, -1.8539, -0.6725, -3.3719,\n",
       "            2.6219],\n",
       "          [-2.3194,  1.2133,  1.3484,  2.6591,  0.3236, -0.9400,  1.0793,\n",
       "           -0.3700],\n",
       "          [-1.9524, -3.0155, -3.0551, -2.8790, -3.2100,  2.8289, -4.8605,\n",
       "            0.1226]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerDecoderLayer(config)\n",
    "print(layer)\n",
    "\n",
    "output = layer(input, cu_seqlens=cu_seqlens)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderBlock(\n",
      "  (vocab_emb): ParallelVocabEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerDecoderLayer(\n",
      "      (attn_pre_norm): GroupRMSNorm()\n",
      "      (rope): NTKAwareRoPE()\n",
      "      (attn): OfflineSlidingWindowAttn(\n",
      "        (softmax_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp_pre_norm): GroupRMSNorm()\n",
      "      (mlp): DenseMLPWithLoRA()\n",
      "    )\n",
      "  )\n",
      "  (kv_cache): TransformerDecoderKVCache()\n",
      "  (final_norm): GroupRMSNorm()\n",
      "  (lm_head): Linear(in_features=8, out_features=10, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 10]),\n",
       " tensor([[[-3.9713,  0.0218,  0.5872,  2.9859,  2.2806,  0.0770,  0.2449,\n",
       "            2.5042,  1.5278,  1.5588],\n",
       "          [-7.4429, -1.5518,  0.5519,  0.6168,  4.0759, -0.4247,  1.1969,\n",
       "            3.2529, -0.6715, -0.7215],\n",
       "          [ 5.8300,  2.5819,  0.0981, -1.9704, -4.6511,  0.2711, -1.7529,\n",
       "           -2.3114,  0.1161, -1.5156],\n",
       "          [ 5.1277,  3.0218,  0.2595, -1.5033, -4.5942,  0.2958, -2.0625,\n",
       "           -1.4238,  0.5330, -1.2860],\n",
       "          [ 5.4769,  1.9154, -0.4239, -1.9101, -3.6879,  0.3206, -1.1926,\n",
       "           -2.5624, -0.2108, -1.1517],\n",
       "          [ 5.1345,  2.0056,  0.1201, -1.4434, -4.0380, -0.0994, -1.7106,\n",
       "           -1.7382,  0.4382, -0.8064]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerDecoderBlock(config)\n",
    "print(block)\n",
    "\n",
    "output = block(input_ids, cu_seqlens=cu_seqlens)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True False\n",
      "'Cache for layer 0 does not exist.'\n",
      "(tensor([[[-0.0166, -0.4668,  2.0909,  0.6149],\n",
      "         [ 0.3083, -0.2947, -0.7662, -0.9962],\n",
      "         [-0.2345, -0.5367,  1.1296,  0.1054],\n",
      "         [-0.3630,  1.5822, -0.4430,  1.8462],\n",
      "         [-0.4772, -1.8291, -0.6145,  1.0282]]]), tensor([[[-0.7771, -0.4484, -1.1668,  0.5006],\n",
      "         [ 0.0139,  0.6564,  0.4846, -0.2549],\n",
      "         [-0.7603, -1.6943, -0.2596,  0.8847],\n",
      "         [-0.8256,  0.7988, -0.3005, -0.3062],\n",
      "         [ 0.4163, -0.5947, -0.2367, -1.8343]]]), None)\n",
      "Layer index must be less than 3 and greater than or equal to 0, but got 3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0166, -0.4668,  2.0909,  0.6149],\n",
       "          [ 0.3083, -0.2947, -0.7662, -0.9962],\n",
       "          [-0.2345, -0.5367,  1.1296,  0.1054],\n",
       "          [-0.3630,  1.5822, -0.4430,  1.8462],\n",
       "          [-0.4772, -1.8291, -0.6145,  1.0282],\n",
       "          [ 1.6024, -0.0655,  1.1773,  0.2308]]]),\n",
       " tensor([[[-0.7771, -0.4484, -1.1668,  0.5006],\n",
       "          [ 0.0139,  0.6564,  0.4846, -0.2549],\n",
       "          [-0.7603, -1.6943, -0.2596,  0.8847],\n",
       "          [-0.8256,  0.7988, -0.3005, -0.3062],\n",
       "          [ 0.4163, -0.5947, -0.2367, -1.8343],\n",
       "          [ 0.6967, -1.3385, -1.3070, -0.4712]]]),\n",
       " None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache = TransformerDecoderKVCache(qkv_layout=AttnQKVLayout.BSHD, num_layers=3)\n",
    "kv_cache.set(1, torch.randn(1, 5, 4), torch.randn(1, 5, 4))\n",
    "\n",
    "print(kv_cache.has(0), kv_cache.has(1), kv_cache.has(2))\n",
    "try: print(kv_cache.get(0))\n",
    "except Exception as e: print(e)\n",
    "print(kv_cache.get(1))\n",
    "try: print(kv_cache.get(3))\n",
    "except Exception as e: print(e)\n",
    "\n",
    "kv_cache.append(1, torch.randn(1, 1, 4), torch.randn(1, 1, 4))\n",
    "kv_cache.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True False\n",
      "'Cache for layer 0 does not exist.'\n",
      "(tensor([[-2.5979, -0.0450, -0.7643, -0.4835],\n",
      "        [-0.7352,  0.6240,  2.3729,  1.3071],\n",
      "        [-0.4575, -0.2238,  0.9910,  1.3958],\n",
      "        [-0.0791, -0.2089, -0.3442,  1.8142],\n",
      "        [ 1.5057,  0.9148, -0.8651, -1.2858],\n",
      "        [-0.3012,  0.3881,  0.6736, -2.2534],\n",
      "        [-0.7496,  1.4628, -2.7743,  1.0224]]), tensor([[-2.7623,  0.8921,  0.3671,  0.5021],\n",
      "        [-0.9558, -2.3803, -0.5387,  1.0196],\n",
      "        [-0.9873, -0.6070, -0.9646,  0.9868],\n",
      "        [ 0.1422,  0.2733, -0.4433, -0.5858],\n",
      "        [-0.5670,  1.2877,  0.2881, -0.0710],\n",
      "        [ 0.6117,  0.5042, -0.3404, -1.3312],\n",
      "        [ 1.3698,  0.3584, -0.5757, -0.2979]]), tensor([0, 2, 3, 7]))\n",
      "Layer index must be less than 3 and greater than or equal to 0, but got 3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.5979, -0.0450, -0.7643, -0.4835],\n",
       "         [-0.7352,  0.6240,  2.3729,  1.3071],\n",
       "         [-1.8024,  1.1348,  0.9362,  1.0182],\n",
       "         [-0.4575, -0.2238,  0.9910,  1.3958],\n",
       "         [-0.7009,  0.2459, -0.1850,  0.6825],\n",
       "         [-0.0791, -0.2089, -0.3442,  1.8142],\n",
       "         [ 1.5057,  0.9148, -0.8651, -1.2858],\n",
       "         [-0.3012,  0.3881,  0.6736, -2.2534],\n",
       "         [-0.7496,  1.4628, -2.7743,  1.0224],\n",
       "         [-0.3955,  0.8117,  0.9530, -0.6530]]),\n",
       " tensor([[-2.7623,  0.8921,  0.3671,  0.5021],\n",
       "         [-0.9558, -2.3803, -0.5387,  1.0196],\n",
       "         [-0.8325, -1.4271, -0.6519, -0.2329],\n",
       "         [-0.9873, -0.6070, -0.9646,  0.9868],\n",
       "         [ 0.4037,  0.8213, -0.1352,  0.3902],\n",
       "         [ 0.1422,  0.2733, -0.4433, -0.5858],\n",
       "         [-0.5670,  1.2877,  0.2881, -0.0710],\n",
       "         [ 0.6117,  0.5042, -0.3404, -1.3312],\n",
       "         [ 1.3698,  0.3584, -0.5757, -0.2979],\n",
       "         [ 0.2317, -0.9516,  0.9047,  0.2141]]),\n",
       " tensor([ 0,  3,  5, 10]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache = TransformerDecoderKVCache(qkv_layout=AttnQKVLayout.THD, num_layers=3)\n",
    "kv_cache.set(1, torch.randn(7, 4), torch.randn(7, 4), cu_seqlens=torch.tensor([0, 2, 3, 7]))\n",
    "\n",
    "print(kv_cache.has(0), kv_cache.has(1), kv_cache.has(2))\n",
    "try: print(kv_cache.get(0))\n",
    "except Exception as e: print(e)\n",
    "print(kv_cache.get(1))\n",
    "try: print(kv_cache.get(3))\n",
    "except Exception as e: print(e)\n",
    "\n",
    "kv_cache.append(1, torch.randn(3, 4), torch.randn(3, 4), cu_seqlens=torch.tensor([0, 1, 2, 3]))\n",
    "kv_cache.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
